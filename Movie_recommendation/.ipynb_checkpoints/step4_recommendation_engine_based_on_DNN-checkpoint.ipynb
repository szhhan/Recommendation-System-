{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>tmdbId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>862</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>862</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>862</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>862</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>862</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  tmdbId  rating\n",
       "0       1     862     4.0\n",
       "1       5     862     4.0\n",
       "2       7     862     4.5\n",
       "3      15     862     2.5\n",
       "4      17     862     4.5"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/rating.csv\")[['userId','tmdbId','rating']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "userCol=\"userId\"\n",
    "productCol=\"tmdbId\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_2_product = {i:j for i,j in enumerate(df[productCol].unique())}\n",
    "product_2_id = {j:i for i,j in id_2_product.items()}\n",
    "\n",
    "id_2_user = {i:j for i,j in enumerate(df[userCol].unique())}\n",
    "user_2_id = {j:i for i,j in id_2_user.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[productCol] = df.apply(lambda x:product_2_id[x[productCol]],axis=1)\n",
    "df[userCol] = df.apply(lambda x:user_2_id[x[userCol]],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[userCol,productCol]].values\n",
    "Y = df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X[:,0].reshape(-1,1)\n",
    "X2 = X[:,1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set up configuration for modeling\"\"\"\n",
    "NUM_PRODUCTS = np.unique(X[:,1]).size\n",
    "NUM_USERS = np.unique(X[:,0]).size\n",
    "DIM_FEATURES = 10\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 512\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique products : 9715, num of unique customers: 610\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num of unique products : {NUM_PRODUCTS}, num of unique customers: {NUM_USERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Regression version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input layer\n",
    "input_user = layers.Input(shape=(1,))\n",
    "input_product = layers.Input(shape=(1,))\n",
    "in_layers = [input_user,input_product]\n",
    "\n",
    "### Embedding layer\n",
    "embed_user = layers.Embedding(NUM_USERS,DIM_FEATURES)(input_user)\n",
    "embed_product = layers.Embedding(NUM_PRODUCTS,DIM_FEATURES)(input_product)\n",
    "\n",
    "### flattern layer \n",
    "flat_user = layers.Flatten()(embed_user)\n",
    "flat_product = layers.Flatten()(embed_product)\n",
    "\n",
    "### Dot layer\n",
    "dot_value = layers.dot([flat_user,flat_product],axes=1,normalize=True)\n",
    "\n",
    "## Dense layer to get y\n",
    "rating_y = layers.Dense(1,kernel_initializer='normal')(dot_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zili/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "### Construct model\n",
    "rating_model = Model(inputs=in_layers,output=rating_y)\n",
    "rating_model.compile(loss=\"mse\",optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_75 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_76 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_63 (Embedding)        (None, 1, 10)        6100        input_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_64 (Embedding)        (None, 1, 10)        97150       input_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_37 (Flatten)            (None, 10)           0           embedding_63[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_38 (Flatten)            (None, 10)           0           embedding_64[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_32 (Dot)                    (None, 1)            0           flatten_37[0][0]                 \n",
      "                                                                 flatten_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 1)            2           dot_32[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 103,252\n",
      "Trainable params: 103,252\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rating_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70576 samples, validate on 30247 samples\n",
      "Epoch 1/200\n",
      "70576/70576 [==============================] - 1s 17us/step - loss: 13.3082 - val_loss: 11.2970\n",
      "Epoch 2/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 10.6196 - val_loss: 10.5411\n",
      "Epoch 3/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 8.4576 - val_loss: 9.8566\n",
      "Epoch 4/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 7.0218 - val_loss: 9.2305\n",
      "Epoch 5/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 5.8604 - val_loss: 8.6591\n",
      "Epoch 6/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 4.8968 - val_loss: 8.1404\n",
      "Epoch 7/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 4.0956 - val_loss: 7.6708\n",
      "Epoch 8/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 3.4324 - val_loss: 7.2487\n",
      "Epoch 9/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 2.8883 - val_loss: 6.8689\n",
      "Epoch 10/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 2.4466 - val_loss: 6.5301\n",
      "Epoch 11/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 2.0910 - val_loss: 6.2287\n",
      "Epoch 12/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 1.8080 - val_loss: 5.9588\n",
      "Epoch 13/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 1.5833 - val_loss: 5.7225\n",
      "Epoch 14/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 1.4014 - val_loss: 5.5087\n",
      "Epoch 15/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 1.2537 - val_loss: 5.3199\n",
      "Epoch 16/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 1.1339 - val_loss: 5.1486\n",
      "Epoch 17/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 1.0373 - val_loss: 4.9948\n",
      "Epoch 18/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.9584 - val_loss: 4.8559\n",
      "Epoch 19/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.8935 - val_loss: 4.7287\n",
      "Epoch 20/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.8393 - val_loss: 4.6155\n",
      "Epoch 21/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.7936 - val_loss: 4.5061\n",
      "Epoch 22/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.7541 - val_loss: 4.4095\n",
      "Epoch 23/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.7195 - val_loss: 4.3150\n",
      "Epoch 24/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.6887 - val_loss: 4.2256\n",
      "Epoch 25/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.6616 - val_loss: 4.1422\n",
      "Epoch 26/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.6367 - val_loss: 4.0639\n",
      "Epoch 27/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.6141 - val_loss: 3.9884\n",
      "Epoch 28/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.5938 - val_loss: 3.9171\n",
      "Epoch 29/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.5749 - val_loss: 3.8496\n",
      "Epoch 30/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.5577 - val_loss: 3.7834\n",
      "Epoch 31/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.5418 - val_loss: 3.7208\n",
      "Epoch 32/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.5275 - val_loss: 3.6620\n",
      "Epoch 33/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.5141 - val_loss: 3.6051\n",
      "Epoch 34/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.5017 - val_loss: 3.5520\n",
      "Epoch 35/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.4905 - val_loss: 3.5008\n",
      "Epoch 36/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.4800 - val_loss: 3.4523\n",
      "Epoch 37/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.4705 - val_loss: 3.4066\n",
      "Epoch 38/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.4620 - val_loss: 3.3623\n",
      "Epoch 39/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.4542 - val_loss: 3.3234\n",
      "Epoch 40/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.4471 - val_loss: 3.2862\n",
      "Epoch 41/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.4407 - val_loss: 3.2497\n",
      "Epoch 42/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.4348 - val_loss: 3.2161\n",
      "Epoch 43/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.4293 - val_loss: 3.1853\n",
      "Epoch 44/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.4246 - val_loss: 3.1572\n",
      "Epoch 45/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.4203 - val_loss: 3.1319\n",
      "Epoch 46/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.4160 - val_loss: 3.1085\n",
      "Epoch 47/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.4126 - val_loss: 3.0849\n",
      "Epoch 48/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.4092 - val_loss: 3.0666\n",
      "Epoch 49/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.4063 - val_loss: 3.0468\n",
      "Epoch 50/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.4038 - val_loss: 3.0316\n",
      "Epoch 51/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.4015 - val_loss: 3.0139\n",
      "Epoch 52/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3988 - val_loss: 3.0009\n",
      "Epoch 53/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.3969 - val_loss: 2.9861\n",
      "Epoch 54/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3950 - val_loss: 2.9779\n",
      "Epoch 55/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3932 - val_loss: 2.9691\n",
      "Epoch 56/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3919 - val_loss: 2.9575\n",
      "Epoch 57/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.3903 - val_loss: 2.9538\n",
      "Epoch 58/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3890 - val_loss: 2.9423\n",
      "Epoch 59/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3875 - val_loss: 2.9396\n",
      "Epoch 60/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3861 - val_loss: 2.9350\n",
      "Epoch 61/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3851 - val_loss: 2.9262\n",
      "Epoch 62/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.3840 - val_loss: 2.9258\n",
      "Epoch 63/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3828 - val_loss: 2.9167\n",
      "Epoch 64/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3821 - val_loss: 2.9133\n",
      "Epoch 65/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3810 - val_loss: 2.9165\n",
      "Epoch 66/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3802 - val_loss: 2.9085\n",
      "Epoch 67/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3793 - val_loss: 2.9132\n",
      "Epoch 68/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3786 - val_loss: 2.9095\n",
      "Epoch 69/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3778 - val_loss: 2.9062\n",
      "Epoch 70/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3770 - val_loss: 2.9121\n",
      "Epoch 71/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3761 - val_loss: 2.9080\n",
      "Epoch 72/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3755 - val_loss: 2.9101\n",
      "Epoch 73/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3750 - val_loss: 2.9134\n",
      "Epoch 74/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3743 - val_loss: 2.9160\n",
      "Epoch 75/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3736 - val_loss: 2.9118\n",
      "Epoch 76/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3732 - val_loss: 2.9125\n",
      "Epoch 77/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3724 - val_loss: 2.9135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3718 - val_loss: 2.9222\n",
      "Epoch 79/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3715 - val_loss: 2.9213\n",
      "Epoch 80/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3710 - val_loss: 2.9295\n",
      "Epoch 81/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3704 - val_loss: 2.9235\n",
      "Epoch 82/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.3697 - val_loss: 2.9300\n",
      "Epoch 83/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3694 - val_loss: 2.9328\n",
      "Epoch 84/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3692 - val_loss: 2.9411\n",
      "Epoch 85/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3685 - val_loss: 2.9439\n",
      "Epoch 86/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3679 - val_loss: 2.9517\n",
      "Epoch 87/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3676 - val_loss: 2.9480\n",
      "Epoch 88/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3672 - val_loss: 2.9527\n",
      "Epoch 89/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.3668 - val_loss: 2.9525\n",
      "Epoch 90/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 0.3662 - val_loss: 2.9594\n",
      "Epoch 91/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3660 - val_loss: 2.9652\n",
      "Epoch 92/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3655 - val_loss: 2.9720\n",
      "Epoch 93/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3655 - val_loss: 2.9738\n",
      "Epoch 94/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3651 - val_loss: 2.9821\n",
      "Epoch 95/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3646 - val_loss: 2.9806\n",
      "Epoch 96/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3642 - val_loss: 2.9858\n",
      "Epoch 97/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3637 - val_loss: 2.9919\n",
      "Epoch 98/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3635 - val_loss: 2.9930\n",
      "Epoch 99/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3632 - val_loss: 2.9988\n",
      "Epoch 100/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3627 - val_loss: 2.9996\n",
      "Epoch 101/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3628 - val_loss: 3.0064\n",
      "Epoch 102/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3624 - val_loss: 3.0059\n",
      "Epoch 103/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3619 - val_loss: 3.0127\n",
      "Epoch 104/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3618 - val_loss: 3.0184\n",
      "Epoch 105/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3613 - val_loss: 3.0227\n",
      "Epoch 106/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3612 - val_loss: 3.0316\n",
      "Epoch 107/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3608 - val_loss: 3.0317\n",
      "Epoch 108/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3607 - val_loss: 3.0295\n",
      "Epoch 109/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3604 - val_loss: 3.0356\n",
      "Epoch 110/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3600 - val_loss: 3.0403\n",
      "Epoch 111/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3601 - val_loss: 3.0499\n",
      "Epoch 112/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3596 - val_loss: 3.0463\n",
      "Epoch 113/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3596 - val_loss: 3.0514\n",
      "Epoch 114/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3591 - val_loss: 3.0588\n",
      "Epoch 115/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3591 - val_loss: 3.0601\n",
      "Epoch 116/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3587 - val_loss: 3.0622\n",
      "Epoch 117/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3584 - val_loss: 3.0662\n",
      "Epoch 118/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3582 - val_loss: 3.0672\n",
      "Epoch 119/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3583 - val_loss: 3.0747\n",
      "Epoch 120/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3576 - val_loss: 3.0737\n",
      "Epoch 121/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3577 - val_loss: 3.0787\n",
      "Epoch 122/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3575 - val_loss: 3.0882\n",
      "Epoch 123/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3572 - val_loss: 3.0892\n",
      "Epoch 124/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3570 - val_loss: 3.0870\n",
      "Epoch 125/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3567 - val_loss: 3.0933\n",
      "Epoch 126/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3565 - val_loss: 3.1010\n",
      "Epoch 127/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3563 - val_loss: 3.1090\n",
      "Epoch 128/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3563 - val_loss: 3.1082\n",
      "Epoch 129/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3559 - val_loss: 3.1090\n",
      "Epoch 130/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3557 - val_loss: 3.1117\n",
      "Epoch 131/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3557 - val_loss: 3.1219\n",
      "Epoch 132/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3556 - val_loss: 3.1213\n",
      "Epoch 133/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3552 - val_loss: 3.1305\n",
      "Epoch 134/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3552 - val_loss: 3.1259\n",
      "Epoch 135/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3550 - val_loss: 3.1367\n",
      "Epoch 136/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3548 - val_loss: 3.1383\n",
      "Epoch 137/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3544 - val_loss: 3.1386\n",
      "Epoch 138/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3544 - val_loss: 3.1401\n",
      "Epoch 139/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3544 - val_loss: 3.1452\n",
      "Epoch 140/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3542 - val_loss: 3.1489\n",
      "Epoch 141/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3540 - val_loss: 3.1468\n",
      "Epoch 142/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3539 - val_loss: 3.1614\n",
      "Epoch 143/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3536 - val_loss: 3.1585\n",
      "Epoch 144/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3535 - val_loss: 3.1634\n",
      "Epoch 145/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3534 - val_loss: 3.1647\n",
      "Epoch 146/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3531 - val_loss: 3.1658\n",
      "Epoch 147/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3530 - val_loss: 3.1727\n",
      "Epoch 148/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3527 - val_loss: 3.1746\n",
      "Epoch 149/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3526 - val_loss: 3.1765\n",
      "Epoch 150/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3524 - val_loss: 3.1832\n",
      "Epoch 151/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3524 - val_loss: 3.1873\n",
      "Epoch 152/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3524 - val_loss: 3.1915\n",
      "Epoch 153/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3522 - val_loss: 3.1969\n",
      "Epoch 154/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3519 - val_loss: 3.1930\n",
      "Epoch 155/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3519 - val_loss: 3.1990\n",
      "Epoch 156/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3517 - val_loss: 3.2060\n",
      "Epoch 157/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3518 - val_loss: 3.2058\n",
      "Epoch 158/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3515 - val_loss: 3.2110\n",
      "Epoch 159/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3513 - val_loss: 3.2161\n",
      "Epoch 160/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3511 - val_loss: 3.2141\n",
      "Epoch 161/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3512 - val_loss: 3.2221\n",
      "Epoch 162/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3510 - val_loss: 3.2235\n",
      "Epoch 163/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3508 - val_loss: 3.2273\n",
      "Epoch 164/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3507 - val_loss: 3.2270\n",
      "Epoch 165/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3506 - val_loss: 3.2321\n",
      "Epoch 166/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3505 - val_loss: 3.2340\n",
      "Epoch 167/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3505 - val_loss: 3.2389\n",
      "Epoch 168/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3502 - val_loss: 3.2447\n",
      "Epoch 169/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3502 - val_loss: 3.2495\n",
      "Epoch 170/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3499 - val_loss: 3.2483\n",
      "Epoch 171/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3498 - val_loss: 3.2539\n",
      "Epoch 172/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3498 - val_loss: 3.2579\n",
      "Epoch 173/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3496 - val_loss: 3.2598\n",
      "Epoch 174/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3495 - val_loss: 3.2629\n",
      "Epoch 175/200\n",
      "70576/70576 [==============================] - 0s 4us/step - loss: 0.3494 - val_loss: 3.2662\n",
      "Epoch 176/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3494 - val_loss: 3.2699\n",
      "Epoch 177/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3492 - val_loss: 3.2738\n",
      "Epoch 178/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3491 - val_loss: 3.2758\n",
      "Epoch 179/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3490 - val_loss: 3.2736\n",
      "Epoch 180/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3489 - val_loss: 3.2818\n",
      "Epoch 181/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3487 - val_loss: 3.2846\n",
      "Epoch 182/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3486 - val_loss: 3.2870\n",
      "Epoch 183/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3486 - val_loss: 3.2885\n",
      "Epoch 184/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3485 - val_loss: 3.2907\n",
      "Epoch 185/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3484 - val_loss: 3.2983\n",
      "Epoch 186/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3484 - val_loss: 3.3054\n",
      "Epoch 187/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3482 - val_loss: 3.2997\n",
      "Epoch 188/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3481 - val_loss: 3.3083\n",
      "Epoch 189/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3481 - val_loss: 3.3111\n",
      "Epoch 190/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3479 - val_loss: 3.3080\n",
      "Epoch 191/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3476 - val_loss: 3.3134\n",
      "Epoch 192/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3477 - val_loss: 3.3220\n",
      "Epoch 193/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3476 - val_loss: 3.3213\n",
      "Epoch 194/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3475 - val_loss: 3.3222\n",
      "Epoch 195/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3473 - val_loss: 3.3275\n",
      "Epoch 196/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3472 - val_loss: 3.3312\n",
      "Epoch 197/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3473 - val_loss: 3.3367\n",
      "Epoch 198/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3470 - val_loss: 3.3360\n",
      "Epoch 199/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3468 - val_loss: 3.3377\n",
      "Epoch 200/200\n",
      "70576/70576 [==============================] - 0s 5us/step - loss: 0.3470 - val_loss: 3.3456\n"
     ]
    }
   ],
   "source": [
    "history = rating_model.fit(x=[X1,X2],y=Y,validation_split=0.3,epochs=EPOCHS,batch_size=BATCH_SIZE,verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = history.history[metric_name]\n",
    "val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "e = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "pred_X1 = np.array([user_id]*NUM_PRODUCTS).reshape(-1,1)\n",
    "pred_X2 = np.array([i for i in range(NUM_PRODUCTS)]).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.0210667, 5.925191 , 5.916854 ], dtype=float32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings = rating_model.predict([pred_X1,pred_X2]).reshape(-1)\n",
    "\n",
    "top_3_index = user_ratings.argsort()[-3:][::-1]\n",
    "\n",
    "user_ratings[top_3_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Accuracy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Generate New Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[[userCol,productCol]].values\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(df['rating'].values)\n",
    "Y = to_categorical(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Generate new classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_FEATURES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input layer\n",
    "input_user = layers.Input(shape=(1,))\n",
    "input_product = layers.Input(shape=(1,))\n",
    "in_layers = [input_user,input_product]\n",
    "\n",
    "### Embedding layer\n",
    "embed_user = layers.Embedding(NUM_USERS,DIM_FEATURES)(input_user)\n",
    "embed_product = layers.Embedding(NUM_PRODUCTS,DIM_FEATURES)(input_product)\n",
    "\n",
    "### flattern layer \n",
    "flat_user = layers.Flatten()(embed_user)\n",
    "flat_product = layers.Flatten()(embed_product)\n",
    "\n",
    "### Dot layer\n",
    "dot_value = layers.dot([flat_user,flat_product],axes=1,normalize=True)\n",
    "\n",
    "dense1 = layers.Dense(20)(dot_value)\n",
    "dense2 = layers.Dense(10)(dense1)\n",
    "\n",
    "rating_y = layers.Dense(10,activation=\"softmax\")(dense2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zili/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "### Construct model\n",
    "rating_model = Model(inputs=in_layers,output=rating_y)\n",
    "rating_model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70576 samples, validate on 30247 samples\n",
      "Epoch 1/200\n",
      "70576/70576 [==============================] - 1s 17us/step - loss: 2.1154 - acc: 0.2546 - val_loss: 2.1165 - val_acc: 0.2419\n",
      "Epoch 2/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.9350 - acc: 0.2767 - val_loss: 2.1189 - val_acc: 0.2388\n",
      "Epoch 3/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.8094 - acc: 0.2909 - val_loss: 2.2095 - val_acc: 0.1986\n",
      "Epoch 4/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.6953 - acc: 0.3183 - val_loss: 2.3852 - val_acc: 0.1928\n",
      "Epoch 5/200\n",
      "70576/70576 [==============================] - ETA: 0s - loss: 1.6095 - acc: 0.347 - 1s 8us/step - loss: 1.6131 - acc: 0.3460 - val_loss: 2.5691 - val_acc: 0.1908\n",
      "Epoch 6/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.5509 - acc: 0.3784 - val_loss: 2.7587 - val_acc: 0.1889\n",
      "Epoch 7/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.5053 - acc: 0.4014 - val_loss: 2.9238 - val_acc: 0.1873\n",
      "Epoch 8/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.4675 - acc: 0.4202 - val_loss: 3.0999 - val_acc: 0.1859\n",
      "Epoch 9/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.4392 - acc: 0.4329 - val_loss: 3.2534 - val_acc: 0.1847\n",
      "Epoch 10/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.4150 - acc: 0.4453 - val_loss: 3.3718 - val_acc: 0.1850\n",
      "Epoch 11/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.3949 - acc: 0.4544 - val_loss: 3.4929 - val_acc: 0.1854\n",
      "Epoch 12/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.3769 - acc: 0.4633 - val_loss: 3.6099 - val_acc: 0.1832\n",
      "Epoch 13/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.3616 - acc: 0.4699 - val_loss: 3.7150 - val_acc: 0.1834\n",
      "Epoch 14/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.3482 - acc: 0.4762 - val_loss: 3.8077 - val_acc: 0.1832\n",
      "Epoch 15/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.3367 - acc: 0.4827 - val_loss: 3.8792 - val_acc: 0.1823\n",
      "Epoch 16/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.3258 - acc: 0.4855 - val_loss: 3.9759 - val_acc: 0.1818\n",
      "Epoch 17/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.3164 - acc: 0.4893 - val_loss: 4.0500 - val_acc: 0.1818\n",
      "Epoch 18/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.3069 - acc: 0.4930 - val_loss: 4.1198 - val_acc: 0.1828\n",
      "Epoch 19/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2986 - acc: 0.4962 - val_loss: 4.1959 - val_acc: 0.1796\n",
      "Epoch 20/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2912 - acc: 0.5006 - val_loss: 4.2635 - val_acc: 0.1802\n",
      "Epoch 21/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2842 - acc: 0.5035 - val_loss: 4.3278 - val_acc: 0.1822\n",
      "Epoch 22/200\n",
      "70576/70576 [==============================] - 1s 7us/step - loss: 1.2774 - acc: 0.5064 - val_loss: 4.3857 - val_acc: 0.1816\n",
      "Epoch 23/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2715 - acc: 0.5077 - val_loss: 4.4526 - val_acc: 0.1823\n",
      "Epoch 24/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2657 - acc: 0.5096 - val_loss: 4.5139 - val_acc: 0.1805\n",
      "Epoch 25/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2600 - acc: 0.5113 - val_loss: 4.5837 - val_acc: 0.1784\n",
      "Epoch 26/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2550 - acc: 0.5144 - val_loss: 4.6193 - val_acc: 0.1790\n",
      "Epoch 27/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2506 - acc: 0.5152 - val_loss: 4.6843 - val_acc: 0.1772\n",
      "Epoch 28/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2462 - acc: 0.5166 - val_loss: 4.7229 - val_acc: 0.1770\n",
      "Epoch 29/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2414 - acc: 0.5187 - val_loss: 4.7865 - val_acc: 0.1741\n",
      "Epoch 30/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2376 - acc: 0.5203 - val_loss: 4.8298 - val_acc: 0.1781\n",
      "Epoch 31/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2328 - acc: 0.5228 - val_loss: 4.8995 - val_acc: 0.1756\n",
      "Epoch 32/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2291 - acc: 0.5244 - val_loss: 4.9430 - val_acc: 0.1764\n",
      "Epoch 33/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2249 - acc: 0.5252 - val_loss: 4.9782 - val_acc: 0.1747\n",
      "Epoch 34/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2220 - acc: 0.5270 - val_loss: 5.0392 - val_acc: 0.1735\n",
      "Epoch 35/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2182 - acc: 0.5277 - val_loss: 5.0722 - val_acc: 0.1744\n",
      "Epoch 36/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2143 - acc: 0.5289 - val_loss: 5.1316 - val_acc: 0.1738\n",
      "Epoch 37/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2111 - acc: 0.5309 - val_loss: 5.1712 - val_acc: 0.1724\n",
      "Epoch 38/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2090 - acc: 0.5318 - val_loss: 5.2030 - val_acc: 0.1729\n",
      "Epoch 39/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2056 - acc: 0.5333 - val_loss: 5.2292 - val_acc: 0.1699\n",
      "Epoch 40/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.2022 - acc: 0.5329 - val_loss: 5.2866 - val_acc: 0.1716\n",
      "Epoch 41/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1996 - acc: 0.5361 - val_loss: 5.3307 - val_acc: 0.1706\n",
      "Epoch 42/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1971 - acc: 0.5364 - val_loss: 5.3377 - val_acc: 0.1739\n",
      "Epoch 43/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1948 - acc: 0.5367 - val_loss: 5.3907 - val_acc: 0.1709\n",
      "Epoch 44/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1916 - acc: 0.5379 - val_loss: 5.4197 - val_acc: 0.1698\n",
      "Epoch 45/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1894 - acc: 0.5382 - val_loss: 5.4486 - val_acc: 0.1718\n",
      "Epoch 46/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1867 - acc: 0.5394 - val_loss: 5.5026 - val_acc: 0.1679\n",
      "Epoch 47/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1844 - acc: 0.5397 - val_loss: 5.5426 - val_acc: 0.1702\n",
      "Epoch 48/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1819 - acc: 0.5411 - val_loss: 5.5829 - val_acc: 0.1685\n",
      "Epoch 49/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1794 - acc: 0.5428 - val_loss: 5.6293 - val_acc: 0.1689\n",
      "Epoch 50/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1779 - acc: 0.5434 - val_loss: 5.6201 - val_acc: 0.1716\n",
      "Epoch 51/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1753 - acc: 0.5438 - val_loss: 5.6894 - val_acc: 0.1699\n",
      "Epoch 52/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1734 - acc: 0.5446 - val_loss: 5.7120 - val_acc: 0.1688\n",
      "Epoch 53/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1709 - acc: 0.5451 - val_loss: 5.7345 - val_acc: 0.1707\n",
      "Epoch 54/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1709 - acc: 0.5455 - val_loss: 5.7646 - val_acc: 0.1697\n",
      "Epoch 55/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1674 - acc: 0.5457 - val_loss: 5.7835 - val_acc: 0.1694\n",
      "Epoch 56/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1655 - acc: 0.5468 - val_loss: 5.8147 - val_acc: 0.1707\n",
      "Epoch 57/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1634 - acc: 0.5473 - val_loss: 5.8388 - val_acc: 0.1707\n",
      "Epoch 58/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1609 - acc: 0.5486 - val_loss: 5.8855 - val_acc: 0.1684\n",
      "Epoch 59/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1587 - acc: 0.5491 - val_loss: 5.9268 - val_acc: 0.1695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1576 - acc: 0.5497 - val_loss: 5.9322 - val_acc: 0.1695\n",
      "Epoch 61/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1561 - acc: 0.5495 - val_loss: 5.9607 - val_acc: 0.1697\n",
      "Epoch 62/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1542 - acc: 0.5520 - val_loss: 5.9989 - val_acc: 0.1661\n",
      "Epoch 63/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1528 - acc: 0.5511 - val_loss: 6.0245 - val_acc: 0.1687\n",
      "Epoch 64/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1509 - acc: 0.5520 - val_loss: 6.0337 - val_acc: 0.1692\n",
      "Epoch 65/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1497 - acc: 0.5522 - val_loss: 6.0909 - val_acc: 0.1687\n",
      "Epoch 66/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1477 - acc: 0.5534 - val_loss: 6.0929 - val_acc: 0.1687\n",
      "Epoch 67/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1459 - acc: 0.5528 - val_loss: 6.1097 - val_acc: 0.1665\n",
      "Epoch 68/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1447 - acc: 0.5533 - val_loss: 6.1619 - val_acc: 0.1690\n",
      "Epoch 69/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1432 - acc: 0.5542 - val_loss: 6.1560 - val_acc: 0.1684\n",
      "Epoch 70/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1413 - acc: 0.5535 - val_loss: 6.1980 - val_acc: 0.1681\n",
      "Epoch 71/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1395 - acc: 0.5551 - val_loss: 6.2403 - val_acc: 0.1654\n",
      "Epoch 72/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1387 - acc: 0.5560 - val_loss: 6.2514 - val_acc: 0.1652\n",
      "Epoch 73/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1380 - acc: 0.5573 - val_loss: 6.2714 - val_acc: 0.1659\n",
      "Epoch 74/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1359 - acc: 0.5562 - val_loss: 6.2738 - val_acc: 0.1673\n",
      "Epoch 75/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1341 - acc: 0.5572 - val_loss: 6.3502 - val_acc: 0.1663\n",
      "Epoch 76/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1333 - acc: 0.5581 - val_loss: 6.3347 - val_acc: 0.1650\n",
      "Epoch 77/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1319 - acc: 0.5574 - val_loss: 6.3988 - val_acc: 0.1644\n",
      "Epoch 78/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1303 - acc: 0.5604 - val_loss: 6.3608 - val_acc: 0.1664\n",
      "Epoch 79/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1290 - acc: 0.5597 - val_loss: 6.4133 - val_acc: 0.1651\n",
      "Epoch 80/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1271 - acc: 0.5619 - val_loss: 6.4317 - val_acc: 0.1646\n",
      "Epoch 81/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1261 - acc: 0.5603 - val_loss: 6.4473 - val_acc: 0.1643\n",
      "Epoch 82/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1255 - acc: 0.5617 - val_loss: 6.4821 - val_acc: 0.1648\n",
      "Epoch 83/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1237 - acc: 0.5630 - val_loss: 6.4871 - val_acc: 0.1658\n",
      "Epoch 84/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1219 - acc: 0.5628 - val_loss: 6.5073 - val_acc: 0.1672\n",
      "Epoch 85/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1209 - acc: 0.5614 - val_loss: 6.5359 - val_acc: 0.1631\n",
      "Epoch 86/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1210 - acc: 0.5641 - val_loss: 6.5555 - val_acc: 0.1627\n",
      "Epoch 87/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1181 - acc: 0.5637 - val_loss: 6.5784 - val_acc: 0.1643\n",
      "Epoch 88/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1184 - acc: 0.5643 - val_loss: 6.5895 - val_acc: 0.1640\n",
      "Epoch 89/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1161 - acc: 0.5650 - val_loss: 6.6104 - val_acc: 0.1636\n",
      "Epoch 90/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1151 - acc: 0.5668 - val_loss: 6.6402 - val_acc: 0.1626\n",
      "Epoch 91/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1137 - acc: 0.5664 - val_loss: 6.6690 - val_acc: 0.1624\n",
      "Epoch 92/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1123 - acc: 0.5681 - val_loss: 6.6623 - val_acc: 0.1623\n",
      "Epoch 93/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1124 - acc: 0.5675 - val_loss: 6.6967 - val_acc: 0.1630\n",
      "Epoch 94/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1112 - acc: 0.5676 - val_loss: 6.7037 - val_acc: 0.1600\n",
      "Epoch 95/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1094 - acc: 0.5693 - val_loss: 6.7228 - val_acc: 0.1629\n",
      "Epoch 96/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1081 - acc: 0.5697 - val_loss: 6.7428 - val_acc: 0.1609\n",
      "Epoch 97/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1071 - acc: 0.5683 - val_loss: 6.7419 - val_acc: 0.1618\n",
      "Epoch 98/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1059 - acc: 0.5699 - val_loss: 6.7591 - val_acc: 0.1612\n",
      "Epoch 99/200\n",
      "70576/70576 [==============================] - 1s 9us/step - loss: 1.1054 - acc: 0.5696 - val_loss: 6.7852 - val_acc: 0.1613\n",
      "Epoch 100/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1037 - acc: 0.5701 - val_loss: 6.8028 - val_acc: 0.1625\n",
      "Epoch 101/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1039 - acc: 0.5703 - val_loss: 6.8030 - val_acc: 0.1572\n",
      "Epoch 102/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1030 - acc: 0.5709 - val_loss: 6.8078 - val_acc: 0.1599\n",
      "Epoch 103/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1001 - acc: 0.5726 - val_loss: 6.8452 - val_acc: 0.1574\n",
      "Epoch 104/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1005 - acc: 0.5722 - val_loss: 6.8548 - val_acc: 0.1607\n",
      "Epoch 105/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.1002 - acc: 0.5730 - val_loss: 6.8579 - val_acc: 0.1625\n",
      "Epoch 106/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0991 - acc: 0.5724 - val_loss: 6.8791 - val_acc: 0.1560\n",
      "Epoch 107/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0965 - acc: 0.5745 - val_loss: 6.9162 - val_acc: 0.1593\n",
      "Epoch 108/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0970 - acc: 0.5740 - val_loss: 6.8921 - val_acc: 0.1576\n",
      "Epoch 109/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0954 - acc: 0.5739 - val_loss: 6.9197 - val_acc: 0.1579\n",
      "Epoch 110/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0946 - acc: 0.5750 - val_loss: 6.9489 - val_acc: 0.1584\n",
      "Epoch 111/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0933 - acc: 0.5746 - val_loss: 6.9710 - val_acc: 0.1610\n",
      "Epoch 112/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0927 - acc: 0.5745 - val_loss: 6.9726 - val_acc: 0.1588\n",
      "Epoch 113/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0918 - acc: 0.5754 - val_loss: 6.9886 - val_acc: 0.1584\n",
      "Epoch 114/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0904 - acc: 0.5760 - val_loss: 6.9737 - val_acc: 0.1585\n",
      "Epoch 115/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0896 - acc: 0.5759 - val_loss: 7.0061 - val_acc: 0.1577\n",
      "Epoch 116/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0895 - acc: 0.5759 - val_loss: 7.0261 - val_acc: 0.1570\n",
      "Epoch 117/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0885 - acc: 0.5768 - val_loss: 7.0557 - val_acc: 0.1583\n",
      "Epoch 118/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0869 - acc: 0.5773 - val_loss: 7.0494 - val_acc: 0.1576\n",
      "Epoch 119/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0870 - acc: 0.5765 - val_loss: 7.0539 - val_acc: 0.1594\n",
      "Epoch 120/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0859 - acc: 0.5774 - val_loss: 7.0447 - val_acc: 0.1575\n",
      "Epoch 121/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0847 - acc: 0.5791 - val_loss: 7.0875 - val_acc: 0.1565\n",
      "Epoch 122/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0851 - acc: 0.5772 - val_loss: 7.0905 - val_acc: 0.1591\n",
      "Epoch 123/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0824 - acc: 0.5790 - val_loss: 7.1251 - val_acc: 0.1571\n",
      "Epoch 124/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0830 - acc: 0.5787 - val_loss: 7.1366 - val_acc: 0.1589\n",
      "Epoch 125/200\n",
      "70576/70576 [==============================] - 1s 7us/step - loss: 1.0819 - acc: 0.5802 - val_loss: 7.0929 - val_acc: 0.1610\n",
      "Epoch 126/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0819 - acc: 0.5790 - val_loss: 7.1327 - val_acc: 0.1574\n",
      "Epoch 127/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0793 - acc: 0.5801 - val_loss: 7.1451 - val_acc: 0.1613\n",
      "Epoch 128/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0799 - acc: 0.5800 - val_loss: 7.1336 - val_acc: 0.1576\n",
      "Epoch 129/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0781 - acc: 0.5803 - val_loss: 7.1610 - val_acc: 0.1585\n",
      "Epoch 130/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0771 - acc: 0.5809 - val_loss: 7.1742 - val_acc: 0.1537\n",
      "Epoch 131/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0780 - acc: 0.5804 - val_loss: 7.1882 - val_acc: 0.1551\n",
      "Epoch 132/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0766 - acc: 0.5817 - val_loss: 7.2274 - val_acc: 0.1594\n",
      "Epoch 133/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0758 - acc: 0.5823 - val_loss: 7.1852 - val_acc: 0.1578\n",
      "Epoch 134/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0753 - acc: 0.5824 - val_loss: 7.2510 - val_acc: 0.1565\n",
      "Epoch 135/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0744 - acc: 0.5834 - val_loss: 7.2105 - val_acc: 0.1549\n",
      "Epoch 136/200\n",
      "70576/70576 [==============================] - 1s 9us/step - loss: 1.0740 - acc: 0.5822 - val_loss: 7.2626 - val_acc: 0.1557\n",
      "Epoch 137/200\n",
      "70576/70576 [==============================] - 1s 9us/step - loss: 1.0734 - acc: 0.5835 - val_loss: 7.2137 - val_acc: 0.1584\n",
      "Epoch 138/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0721 - acc: 0.5819 - val_loss: 7.2484 - val_acc: 0.1594\n",
      "Epoch 139/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0721 - acc: 0.5838 - val_loss: 7.2577 - val_acc: 0.1581\n",
      "Epoch 140/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0705 - acc: 0.5842 - val_loss: 7.2764 - val_acc: 0.1570\n",
      "Epoch 141/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0696 - acc: 0.5830 - val_loss: 7.2825 - val_acc: 0.1564\n",
      "Epoch 142/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0681 - acc: 0.5843 - val_loss: 7.2975 - val_acc: 0.1547\n",
      "Epoch 143/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0685 - acc: 0.5847 - val_loss: 7.2940 - val_acc: 0.1573\n",
      "Epoch 144/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0682 - acc: 0.5852 - val_loss: 7.3376 - val_acc: 0.1558\n",
      "Epoch 145/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0676 - acc: 0.5846 - val_loss: 7.3303 - val_acc: 0.1568\n",
      "Epoch 146/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0661 - acc: 0.5844 - val_loss: 7.3353 - val_acc: 0.1555\n",
      "Epoch 147/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0669 - acc: 0.5857 - val_loss: 7.3439 - val_acc: 0.1567\n",
      "Epoch 148/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0652 - acc: 0.5871 - val_loss: 7.3653 - val_acc: 0.1544\n",
      "Epoch 149/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0657 - acc: 0.5857 - val_loss: 7.3638 - val_acc: 0.1573\n",
      "Epoch 150/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0653 - acc: 0.5862 - val_loss: 7.3473 - val_acc: 0.1578\n",
      "Epoch 151/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0633 - acc: 0.5866 - val_loss: 7.3680 - val_acc: 0.1565\n",
      "Epoch 152/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0627 - acc: 0.5882 - val_loss: 7.4098 - val_acc: 0.1552\n",
      "Epoch 153/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0621 - acc: 0.5853 - val_loss: 7.3952 - val_acc: 0.1561\n",
      "Epoch 154/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0610 - acc: 0.5863 - val_loss: 7.3945 - val_acc: 0.1566\n",
      "Epoch 155/200\n",
      "70576/70576 [==============================] - 1s 9us/step - loss: 1.0615 - acc: 0.5880 - val_loss: 7.3925 - val_acc: 0.1567\n",
      "Epoch 156/200\n",
      "70576/70576 [==============================] - 1s 9us/step - loss: 1.0605 - acc: 0.5888 - val_loss: 7.4497 - val_acc: 0.1540\n",
      "Epoch 157/200\n",
      "70576/70576 [==============================] - 1s 9us/step - loss: 1.0600 - acc: 0.5895 - val_loss: 7.3882 - val_acc: 0.1571\n",
      "Epoch 158/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0588 - acc: 0.5890 - val_loss: 7.4599 - val_acc: 0.1542\n",
      "Epoch 159/200\n",
      "70576/70576 [==============================] - 1s 9us/step - loss: 1.0583 - acc: 0.5878 - val_loss: 7.4552 - val_acc: 0.1575\n",
      "Epoch 160/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0574 - acc: 0.5906 - val_loss: 7.4403 - val_acc: 0.1561\n",
      "Epoch 161/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0578 - acc: 0.5888 - val_loss: 7.4937 - val_acc: 0.1551\n",
      "Epoch 162/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0572 - acc: 0.5891 - val_loss: 7.4820 - val_acc: 0.1554\n",
      "Epoch 163/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0565 - acc: 0.5908 - val_loss: 7.4782 - val_acc: 0.1522\n",
      "Epoch 164/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0549 - acc: 0.5924 - val_loss: 7.4952 - val_acc: 0.1552\n",
      "Epoch 165/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0552 - acc: 0.5900 - val_loss: 7.4802 - val_acc: 0.1536\n",
      "Epoch 166/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0555 - acc: 0.5911 - val_loss: 7.4914 - val_acc: 0.1559\n",
      "Epoch 167/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0535 - acc: 0.5913 - val_loss: 7.4776 - val_acc: 0.1558\n",
      "Epoch 168/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0522 - acc: 0.5900 - val_loss: 7.5123 - val_acc: 0.1561\n",
      "Epoch 169/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0539 - acc: 0.5913 - val_loss: 7.5243 - val_acc: 0.1535\n",
      "Epoch 170/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0531 - acc: 0.5908 - val_loss: 7.5315 - val_acc: 0.1528\n",
      "Epoch 171/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0521 - acc: 0.5924 - val_loss: 7.5329 - val_acc: 0.1552\n",
      "Epoch 172/200\n",
      "70576/70576 [==============================] - 0s 7us/step - loss: 1.0512 - acc: 0.5911 - val_loss: 7.5395 - val_acc: 0.1545\n",
      "Epoch 173/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0506 - acc: 0.5928 - val_loss: 7.5566 - val_acc: 0.1554\n",
      "Epoch 174/200\n",
      "70576/70576 [==============================] - 1s 7us/step - loss: 1.0501 - acc: 0.5918 - val_loss: 7.5915 - val_acc: 0.1551\n",
      "Epoch 175/200\n",
      "70576/70576 [==============================] - 1s 7us/step - loss: 1.0495 - acc: 0.5938 - val_loss: 7.5689 - val_acc: 0.1581\n",
      "Epoch 176/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0488 - acc: 0.5931 - val_loss: 7.5499 - val_acc: 0.1562\n",
      "Epoch 177/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0482 - acc: 0.5936 - val_loss: 7.5600 - val_acc: 0.1577\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0488 - acc: 0.5932 - val_loss: 7.5777 - val_acc: 0.1538\n",
      "Epoch 179/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0459 - acc: 0.5950 - val_loss: 7.6199 - val_acc: 0.1532\n",
      "Epoch 180/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0466 - acc: 0.5959 - val_loss: 7.6212 - val_acc: 0.1523\n",
      "Epoch 181/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0472 - acc: 0.5931 - val_loss: 7.5955 - val_acc: 0.1563\n",
      "Epoch 182/200\n",
      "70576/70576 [==============================] - 1s 7us/step - loss: 1.0463 - acc: 0.5946 - val_loss: 7.6166 - val_acc: 0.1571\n",
      "Epoch 183/200\n",
      "70576/70576 [==============================] - 0s 6us/step - loss: 1.0462 - acc: 0.5955 - val_loss: 7.6112 - val_acc: 0.1539\n",
      "Epoch 184/200\n",
      "70576/70576 [==============================] - 1s 7us/step - loss: 1.0452 - acc: 0.5935 - val_loss: 7.5998 - val_acc: 0.1576\n",
      "Epoch 185/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0434 - acc: 0.5960 - val_loss: 7.6305 - val_acc: 0.1519\n",
      "Epoch 186/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0420 - acc: 0.5961 - val_loss: 7.6502 - val_acc: 0.1541\n",
      "Epoch 187/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0429 - acc: 0.5953 - val_loss: 7.6604 - val_acc: 0.1571\n",
      "Epoch 188/200\n",
      "70576/70576 [==============================] - 1s 7us/step - loss: 1.0425 - acc: 0.5956 - val_loss: 7.6224 - val_acc: 0.1570\n",
      "Epoch 189/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0424 - acc: 0.5944 - val_loss: 7.6581 - val_acc: 0.1550\n",
      "Epoch 190/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0421 - acc: 0.5946 - val_loss: 7.6403 - val_acc: 0.1548\n",
      "Epoch 191/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0418 - acc: 0.5946 - val_loss: 7.7061 - val_acc: 0.1560\n",
      "Epoch 192/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0411 - acc: 0.5968 - val_loss: 7.6612 - val_acc: 0.1541\n",
      "Epoch 193/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0401 - acc: 0.5956 - val_loss: 7.6867 - val_acc: 0.1547\n",
      "Epoch 194/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0402 - acc: 0.5976 - val_loss: 7.6592 - val_acc: 0.1538\n",
      "Epoch 195/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0398 - acc: 0.5978 - val_loss: 7.6886 - val_acc: 0.1567\n",
      "Epoch 196/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0388 - acc: 0.5951 - val_loss: 7.6997 - val_acc: 0.1548\n",
      "Epoch 197/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0384 - acc: 0.5988 - val_loss: 7.7257 - val_acc: 0.1545\n",
      "Epoch 198/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0363 - acc: 0.5972 - val_loss: 7.7046 - val_acc: 0.1547\n",
      "Epoch 199/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0365 - acc: 0.5986 - val_loss: 7.7129 - val_acc: 0.1545\n",
      "Epoch 200/200\n",
      "70576/70576 [==============================] - 1s 8us/step - loss: 1.0363 - acc: 0.5977 - val_loss: 7.7215 - val_acc: 0.1540\n"
     ]
    }
   ],
   "source": [
    "history = rating_model.fit(x=[X1,X2],y=Y,validation_split=0.3,epochs=EPOCHS,batch_size=BATCH_SIZE,verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"acc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = history.history[metric_name]\n",
    "val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "e = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Modulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rator:\n",
    "    def __init__(self,num_users,num_products,embed_size=20,pred_type='regression'):\n",
    "        ### input layer\n",
    "        if pred_type not in ['regression','classification']:\n",
    "            raise Exception(\"Must select from regression or classification\")\n",
    "        self.embed_size = embed_size\n",
    "        self.num_users = num_users\n",
    "        self.num_products = num_products\n",
    "        self.pred_type = pred_type\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        input_user = layers.Input(shape=(1,))\n",
    "        input_product = layers.Input(shape=(1,))\n",
    "        in_layers = [input_user,input_product]\n",
    "\n",
    "        ### Embedding layer\n",
    "        embed_user = layers.Embedding(self.num_users ,self.embed_size)(input_user)\n",
    "        embed_product = layers.Embedding(self.num_products,self.embed_size)(input_product)\n",
    "\n",
    "        ### flattern layer \n",
    "        flat_user = layers.Flatten()(embed_user)\n",
    "        flat_product = layers.Flatten()(embed_product)\n",
    "\n",
    "        ### Dot layer\n",
    "        dot_value = layers.dot([flat_user,flat_product],axes=1,normalize=True)\n",
    "\n",
    "        dense1 = layers.Dense(20)(dot_value)\n",
    "        dense2 = layers.Dense(10)(dense1)\n",
    "\n",
    "        if self.pred_type == \"classification\" :\n",
    "            rating_y = layers.Dense(10,activation=\"softmax\")(dense2)\n",
    "        else:\n",
    "            rating_y = layers.Dense(1)(dense2)\n",
    "            \n",
    "        ### Construct model\n",
    "        self.rating_model = Model(inputs=in_layers,output=rating_y)\n",
    "        if self.pred_type == \"classification\":\n",
    "            self.rating_model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "        else:\n",
    "            self.rating_model.compile(loss=\"mse\",optimizer=\"adam\")\n",
    "            \n",
    "    def train(self,x,y,validation_split=0.2,epochs=200,batch_size=256):\n",
    "        self.history = self.rating_model.fit(x=x,y=Y,validation_split=validation_split,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "\n",
    "    def plot(self):\n",
    "        metric_name = \"acc\" if self.pred_type == \"classification\" else \"loss\"\n",
    "        \n",
    "        metric = history.history[metric_name]\n",
    "        val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "        e = range(1, EPOCHS + 1)\n",
    "\n",
    "        plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "        plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def recommend2User(self,userId,top_n=3):\n",
    "\n",
    "        pred_X1 = np.array([userId]*self.num_products).reshape(-1,1)\n",
    "        pred_X2 = np.array([i for i in range(self.num_products)]).reshape(-1,1)\n",
    "\n",
    "        user_ratings = self.rating_model.predict([pred_X1,pred_X2]).reshape(-1)\n",
    "\n",
    "        top_n_index = user_ratings.argsort()[-top_n:][::-1]\n",
    "\n",
    "        top_n_ratings = user_ratings[top_n_index]\n",
    "        \n",
    "        return top_n_index,top_n_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[userCol,productCol]].values\n",
    "Y = df['rating'].values\n",
    "\n",
    "X1 = X[:,0].reshape(-1,1)\n",
    "X2 = X[:,1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set up configuration for modeling\"\"\"\n",
    "NUM_PRODUCTS = np.unique(X[:,1]).size\n",
    "NUM_USERS = np.unique(X[:,0]).size\n",
    "DIM_FEATURES = 10\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 512\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zili/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "myRator = rator(embed_size=20,num_products=NUM_PRODUCTS,num_users=NUM_USERS,pred_type=\"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20164 samples, validate on 80659 samples\n",
      "Epoch 1/200\n",
      "20164/20164 [==============================] - 2s 85us/step - loss: 10.9547 - val_loss: 4.0272\n",
      "Epoch 2/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 1.5077 - val_loss: 1.5127\n",
      "Epoch 3/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.6222 - val_loss: 1.5851\n",
      "Epoch 4/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.5120 - val_loss: 1.6922\n",
      "Epoch 5/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.4364 - val_loss: 1.8036\n",
      "Epoch 6/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.3804 - val_loss: 1.8835\n",
      "Epoch 7/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.3402 - val_loss: 1.9537\n",
      "Epoch 8/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.3104 - val_loss: 2.0082\n",
      "Epoch 9/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.2878 - val_loss: 2.0350\n",
      "Epoch 10/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.2692 - val_loss: 2.0625\n",
      "Epoch 11/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.2545 - val_loss: 2.0941\n",
      "Epoch 12/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.2430 - val_loss: 2.1179\n",
      "Epoch 13/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.2332 - val_loss: 2.1516\n",
      "Epoch 14/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.2245 - val_loss: 2.1598\n",
      "Epoch 15/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.2186 - val_loss: 2.1762\n",
      "Epoch 16/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.2117 - val_loss: 2.1997\n",
      "Epoch 17/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.2057 - val_loss: 2.2248\n",
      "Epoch 18/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.2011 - val_loss: 2.2363\n",
      "Epoch 19/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1968 - val_loss: 2.2387\n",
      "Epoch 20/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1934 - val_loss: 2.2544\n",
      "Epoch 21/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1903 - val_loss: 2.2471\n",
      "Epoch 22/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1865 - val_loss: 2.2689\n",
      "Epoch 23/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1839 - val_loss: 2.2765\n",
      "Epoch 24/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1815 - val_loss: 2.2895\n",
      "Epoch 25/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1785 - val_loss: 2.2972\n",
      "Epoch 26/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1764 - val_loss: 2.3138\n",
      "Epoch 27/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1741 - val_loss: 2.3233\n",
      "Epoch 28/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1715 - val_loss: 2.3307\n",
      "Epoch 29/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1694 - val_loss: 2.3654\n",
      "Epoch 30/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.1685 - val_loss: 2.3404\n",
      "Epoch 31/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1665 - val_loss: 2.3598\n",
      "Epoch 32/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1659 - val_loss: 2.3450\n",
      "Epoch 33/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.1639 - val_loss: 2.3665\n",
      "Epoch 34/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1622 - val_loss: 2.3824\n",
      "Epoch 35/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.1609 - val_loss: 2.3800\n",
      "Epoch 36/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1598 - val_loss: 2.3862\n",
      "Epoch 37/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1587 - val_loss: 2.4094\n",
      "Epoch 38/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1571 - val_loss: 2.3900\n",
      "Epoch 39/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1565 - val_loss: 2.4126\n",
      "Epoch 40/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1556 - val_loss: 2.3710\n",
      "Epoch 41/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1543 - val_loss: 2.4360\n",
      "Epoch 42/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1535 - val_loss: 2.4061\n",
      "Epoch 43/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1524 - val_loss: 2.4263\n",
      "Epoch 44/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1514 - val_loss: 2.4588\n",
      "Epoch 45/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1509 - val_loss: 2.4302\n",
      "Epoch 46/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1495 - val_loss: 2.4631\n",
      "Epoch 47/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1488 - val_loss: 2.4729\n",
      "Epoch 48/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.1480 - val_loss: 2.4319\n",
      "Epoch 49/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1475 - val_loss: 2.4657\n",
      "Epoch 50/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.1464 - val_loss: 2.4555\n",
      "Epoch 51/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.1461 - val_loss: 2.5083\n",
      "Epoch 52/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1457 - val_loss: 2.4738\n",
      "Epoch 53/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1446 - val_loss: 2.5139\n",
      "Epoch 54/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1443 - val_loss: 2.4942\n",
      "Epoch 55/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1445 - val_loss: 2.5435\n",
      "Epoch 56/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1432 - val_loss: 2.5301\n",
      "Epoch 57/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1431 - val_loss: 2.5339\n",
      "Epoch 58/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.1414 - val_loss: 2.5178\n",
      "Epoch 59/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1411 - val_loss: 2.5746\n",
      "Epoch 60/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.1404 - val_loss: 2.5385\n",
      "Epoch 61/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1399 - val_loss: 2.5488\n",
      "Epoch 62/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1399 - val_loss: 2.5422\n",
      "Epoch 63/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1393 - val_loss: 2.5384\n",
      "Epoch 64/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1385 - val_loss: 2.5868\n",
      "Epoch 65/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1383 - val_loss: 2.5686\n",
      "Epoch 66/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1377 - val_loss: 2.5506\n",
      "Epoch 67/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1372 - val_loss: 2.6353\n",
      "Epoch 68/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1373 - val_loss: 2.6072\n",
      "Epoch 69/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.1368 - val_loss: 2.6271\n",
      "Epoch 70/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1368 - val_loss: 2.6066\n",
      "Epoch 71/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1358 - val_loss: 2.6700\n",
      "Epoch 72/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1351 - val_loss: 2.6627\n",
      "Epoch 73/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1351 - val_loss: 2.6286\n",
      "Epoch 74/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1344 - val_loss: 2.6252\n",
      "Epoch 75/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.1343 - val_loss: 2.6563\n",
      "Epoch 76/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1346 - val_loss: 2.6435\n",
      "Epoch 77/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.1332 - val_loss: 2.6864\n",
      "Epoch 78/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1330 - val_loss: 2.7062\n",
      "Epoch 79/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1329 - val_loss: 2.6926\n",
      "Epoch 80/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1321 - val_loss: 2.6918\n",
      "Epoch 81/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.1314 - val_loss: 2.6935\n",
      "Epoch 82/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1316 - val_loss: 2.7075\n",
      "Epoch 83/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1308 - val_loss: 2.6986\n",
      "Epoch 84/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1303 - val_loss: 2.6802\n",
      "Epoch 85/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1308 - val_loss: 2.7508\n",
      "Epoch 86/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1304 - val_loss: 2.7273\n",
      "Epoch 87/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1297 - val_loss: 2.7117\n",
      "Epoch 88/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1295 - val_loss: 2.7159\n",
      "Epoch 89/200\n",
      "20164/20164 [==============================] - 0s 21us/step - loss: 0.1292 - val_loss: 2.7687\n",
      "Epoch 90/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1300 - val_loss: 2.7097\n",
      "Epoch 91/200\n",
      "20164/20164 [==============================] - 0s 20us/step - loss: 0.1289 - val_loss: 2.7794\n",
      "Epoch 92/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1279 - val_loss: 2.7385\n",
      "Epoch 93/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1281 - val_loss: 2.7446\n",
      "Epoch 94/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1278 - val_loss: 2.7592\n",
      "Epoch 95/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1275 - val_loss: 2.8007\n",
      "Epoch 96/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1270 - val_loss: 2.8067\n",
      "Epoch 97/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1266 - val_loss: 2.7902\n",
      "Epoch 98/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1264 - val_loss: 2.7895\n",
      "Epoch 99/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1265 - val_loss: 2.8170\n",
      "Epoch 100/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1262 - val_loss: 2.8291\n",
      "Epoch 101/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1259 - val_loss: 2.8022\n",
      "Epoch 102/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1259 - val_loss: 2.8084\n",
      "Epoch 103/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1258 - val_loss: 2.8320\n",
      "Epoch 104/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1249 - val_loss: 2.8268\n",
      "Epoch 105/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.1246 - val_loss: 2.8510\n",
      "Epoch 106/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1243 - val_loss: 2.8216\n",
      "Epoch 107/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1245 - val_loss: 2.8766\n",
      "Epoch 108/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.1245 - val_loss: 2.8185\n",
      "Epoch 109/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.1238 - val_loss: 2.8761\n",
      "Epoch 110/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1243 - val_loss: 2.8925\n",
      "Epoch 111/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1233 - val_loss: 2.8270\n",
      "Epoch 112/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.1228 - val_loss: 2.8763\n",
      "Epoch 113/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1225 - val_loss: 2.9046\n",
      "Epoch 114/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1218 - val_loss: 2.9132\n",
      "Epoch 115/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1222 - val_loss: 2.9152\n",
      "Epoch 116/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1217 - val_loss: 2.9377\n",
      "Epoch 117/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1221 - val_loss: 2.9045\n",
      "Epoch 118/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1220 - val_loss: 2.8974\n",
      "Epoch 119/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1214 - val_loss: 2.8881\n",
      "Epoch 120/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.1217 - val_loss: 2.8926\n",
      "Epoch 121/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1212 - val_loss: 2.9477\n",
      "Epoch 122/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.1209 - val_loss: 2.9074\n",
      "Epoch 123/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.1215 - val_loss: 2.9157\n",
      "Epoch 124/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1205 - val_loss: 2.8984\n",
      "Epoch 125/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1205 - val_loss: 2.9309\n",
      "Epoch 126/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.1199 - val_loss: 2.9582\n",
      "Epoch 127/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1198 - val_loss: 2.9469\n",
      "Epoch 128/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1195 - val_loss: 2.9566\n",
      "Epoch 129/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1197 - val_loss: 2.9331\n",
      "Epoch 130/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1198 - val_loss: 2.9941\n",
      "Epoch 131/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.1191 - val_loss: 2.9926\n",
      "Epoch 132/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1186 - val_loss: 2.9555\n",
      "Epoch 133/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1187 - val_loss: 2.9706\n",
      "Epoch 134/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1181 - val_loss: 2.9969\n",
      "Epoch 135/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1178 - val_loss: 2.9603\n",
      "Epoch 136/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1179 - val_loss: 3.0555\n",
      "Epoch 137/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1178 - val_loss: 3.0107\n",
      "Epoch 138/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1179 - val_loss: 3.0365\n",
      "Epoch 139/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1179 - val_loss: 3.0311\n",
      "Epoch 140/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1176 - val_loss: 2.9951\n",
      "Epoch 141/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1169 - val_loss: 3.0519\n",
      "Epoch 142/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1169 - val_loss: 3.0372\n",
      "Epoch 143/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1168 - val_loss: 3.0938\n",
      "Epoch 144/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1165 - val_loss: 3.0373\n",
      "Epoch 145/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1162 - val_loss: 3.0600\n",
      "Epoch 146/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1164 - val_loss: 3.0789\n",
      "Epoch 147/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1170 - val_loss: 3.0664\n",
      "Epoch 148/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1160 - val_loss: 3.0893\n",
      "Epoch 149/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1161 - val_loss: 3.0517\n",
      "Epoch 150/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1156 - val_loss: 3.0918\n",
      "Epoch 151/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1150 - val_loss: 3.0897\n",
      "Epoch 152/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1154 - val_loss: 3.0751\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1155 - val_loss: 3.0881\n",
      "Epoch 154/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1151 - val_loss: 3.1173\n",
      "Epoch 155/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1147 - val_loss: 3.0688\n",
      "Epoch 156/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1152 - val_loss: 3.1164\n",
      "Epoch 157/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1150 - val_loss: 3.1206\n",
      "Epoch 158/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1144 - val_loss: 3.1219\n",
      "Epoch 159/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1146 - val_loss: 3.1723\n",
      "Epoch 160/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1144 - val_loss: 3.1378\n",
      "Epoch 161/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1140 - val_loss: 3.1337\n",
      "Epoch 162/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1140 - val_loss: 3.1154\n",
      "Epoch 163/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1142 - val_loss: 3.1690\n",
      "Epoch 164/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1140 - val_loss: 3.1364\n",
      "Epoch 165/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1139 - val_loss: 3.1861\n",
      "Epoch 166/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1133 - val_loss: 3.1636\n",
      "Epoch 167/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1131 - val_loss: 3.1592\n",
      "Epoch 168/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1138 - val_loss: 3.1911\n",
      "Epoch 169/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.1132 - val_loss: 3.1602\n",
      "Epoch 170/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1133 - val_loss: 3.2062\n",
      "Epoch 171/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1129 - val_loss: 3.1820\n",
      "Epoch 172/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1129 - val_loss: 3.2061\n",
      "Epoch 173/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1126 - val_loss: 3.2089\n",
      "Epoch 174/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1122 - val_loss: 3.1575\n",
      "Epoch 175/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.1122 - val_loss: 3.2163\n",
      "Epoch 176/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1119 - val_loss: 3.2119\n",
      "Epoch 177/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1120 - val_loss: 3.2309\n",
      "Epoch 178/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1121 - val_loss: 3.2461\n",
      "Epoch 179/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1119 - val_loss: 3.2026\n",
      "Epoch 180/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1117 - val_loss: 3.1923\n",
      "Epoch 181/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1115 - val_loss: 3.2013\n",
      "Epoch 182/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1114 - val_loss: 3.2357\n",
      "Epoch 183/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1110 - val_loss: 3.2271\n",
      "Epoch 184/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1116 - val_loss: 3.2611\n",
      "Epoch 185/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1114 - val_loss: 3.2317\n",
      "Epoch 186/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1112 - val_loss: 3.1773\n",
      "Epoch 187/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1112 - val_loss: 3.2642\n",
      "Epoch 188/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1111 - val_loss: 3.2321\n",
      "Epoch 189/200\n",
      "20164/20164 [==============================] - 0s 22us/step - loss: 0.1110 - val_loss: 3.2456\n",
      "Epoch 190/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1105 - val_loss: 3.2734\n",
      "Epoch 191/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1101 - val_loss: 3.2357\n",
      "Epoch 192/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1102 - val_loss: 3.2614\n",
      "Epoch 193/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1101 - val_loss: 3.2829\n",
      "Epoch 194/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.1102 - val_loss: 3.3085\n",
      "Epoch 195/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1100 - val_loss: 3.2967\n",
      "Epoch 196/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 0.1104 - val_loss: 3.3373\n",
      "Epoch 197/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1099 - val_loss: 3.2848\n",
      "Epoch 198/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1097 - val_loss: 3.3050\n",
      "Epoch 199/200\n",
      "20164/20164 [==============================] - 0s 23us/step - loss: 0.1091 - val_loss: 3.2652\n",
      "Epoch 200/200\n",
      "20164/20164 [==============================] - 0s 24us/step - loss: 0.1092 - val_loss: 3.3564\n"
     ]
    }
   ],
   "source": [
    "myRator.train(x=[X1,X2],y=Y,batch_size=256,epochs=200,validation_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myRator.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1566, 3269, 1271, 1622, 9249, 3171, 4858, 4700, 9493, 7938]),\n",
       " array([8.04857  , 7.6122055, 7.4882536, 7.384393 , 7.239051 , 7.114005 ,\n",
       "        7.0881624, 7.0768485, 7.052861 , 7.02934  ], dtype=float32))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRator.recommend2User(userId=1,top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Accuracy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zili/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "myRator = rator(embed_size=20,num_products=NUM_PRODUCTS,num_users=NUM_USERS,pred_type=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_91 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_92 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_79 (Embedding)        (None, 1, 20)        12200       input_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_80 (Embedding)        (None, 1, 20)        194300      input_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_53 (Flatten)            (None, 20)           0           embedding_79[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_54 (Flatten)            (None, 20)           0           embedding_80[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_40 (Dot)                    (None, 1)            0           flatten_53[0][0]                 \n",
      "                                                                 flatten_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_71 (Dense)                (None, 20)           40          dot_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 10)           210         dense_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_73 (Dense)                (None, 10)           110         dense_72[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 206,860\n",
      "Trainable params: 206,860\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myRator.rating_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(df['rating'].values)\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20164 samples, validate on 80659 samples\n",
      "Epoch 1/200\n",
      "20164/20164 [==============================] - 2s 101us/step - loss: 2.1357 - acc: 0.2675 - val_loss: 2.0844 - val_acc: 0.2581\n",
      "Epoch 2/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 1.8629 - acc: 0.3004 - val_loss: 2.1078 - val_acc: 0.2493\n",
      "Epoch 3/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 1.7343 - acc: 0.3578 - val_loss: 2.1816 - val_acc: 0.2283\n",
      "Epoch 4/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 1.5818 - acc: 0.4311 - val_loss: 2.3099 - val_acc: 0.2247\n",
      "Epoch 5/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 1.4697 - acc: 0.4752 - val_loss: 2.5135 - val_acc: 0.2233\n",
      "Epoch 6/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 1.3862 - acc: 0.5070 - val_loss: 2.7409 - val_acc: 0.2207\n",
      "Epoch 7/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 1.3243 - acc: 0.5261 - val_loss: 2.9825 - val_acc: 0.2183\n",
      "Epoch 8/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 1.2727 - acc: 0.5448 - val_loss: 3.2029 - val_acc: 0.2170\n",
      "Epoch 9/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 1.2332 - acc: 0.5539 - val_loss: 3.4389 - val_acc: 0.2142\n",
      "Epoch 10/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 1.2002 - acc: 0.5650 - val_loss: 3.6597 - val_acc: 0.2110\n",
      "Epoch 11/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 1.1747 - acc: 0.5750 - val_loss: 3.8544 - val_acc: 0.2097\n",
      "Epoch 12/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 1.1502 - acc: 0.5826 - val_loss: 4.0270 - val_acc: 0.2076\n",
      "Epoch 13/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 1.1320 - acc: 0.5892 - val_loss: 4.2299 - val_acc: 0.2066\n",
      "Epoch 14/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 1.1135 - acc: 0.5951 - val_loss: 4.3483 - val_acc: 0.2054\n",
      "Epoch 15/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 1.0990 - acc: 0.5978 - val_loss: 4.4617 - val_acc: 0.2053\n",
      "Epoch 16/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 1.0852 - acc: 0.6026 - val_loss: 4.5968 - val_acc: 0.2028\n",
      "Epoch 17/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 1.0725 - acc: 0.6051 - val_loss: 4.7053 - val_acc: 0.2032\n",
      "Epoch 18/200\n",
      "20164/20164 [==============================] - 0s 25us/step - loss: 1.0633 - acc: 0.6081 - val_loss: 4.8033 - val_acc: 0.2018\n",
      "Epoch 19/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 1.0527 - acc: 0.6112 - val_loss: 4.9303 - val_acc: 0.1989\n",
      "Epoch 20/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 1.0469 - acc: 0.6143 - val_loss: 5.0061 - val_acc: 0.1974\n",
      "Epoch 21/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 1.0368 - acc: 0.6167 - val_loss: 5.0844 - val_acc: 0.1990\n",
      "Epoch 22/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 1.0286 - acc: 0.6210 - val_loss: 5.1763 - val_acc: 0.1985\n",
      "Epoch 23/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 1.0216 - acc: 0.6217 - val_loss: 5.2524 - val_acc: 0.1965\n",
      "Epoch 24/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 1.0148 - acc: 0.6264 - val_loss: 5.3327 - val_acc: 0.1979\n",
      "Epoch 25/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 1.0085 - acc: 0.6274 - val_loss: 5.3971 - val_acc: 0.1951\n",
      "Epoch 26/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 1.0043 - acc: 0.6279 - val_loss: 5.4662 - val_acc: 0.1934\n",
      "Epoch 27/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.9974 - acc: 0.6327 - val_loss: 5.5333 - val_acc: 0.1960\n",
      "Epoch 28/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.9934 - acc: 0.6319 - val_loss: 5.6133 - val_acc: 0.1922\n",
      "Epoch 29/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.9880 - acc: 0.6366 - val_loss: 5.6324 - val_acc: 0.1926\n",
      "Epoch 30/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.9821 - acc: 0.6343 - val_loss: 5.6999 - val_acc: 0.1917\n",
      "Epoch 31/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.9775 - acc: 0.6384 - val_loss: 5.7684 - val_acc: 0.1950\n",
      "Epoch 32/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.9728 - acc: 0.6391 - val_loss: 5.8235 - val_acc: 0.1894\n",
      "Epoch 33/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.9691 - acc: 0.6421 - val_loss: 5.8887 - val_acc: 0.1888\n",
      "Epoch 34/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.9655 - acc: 0.6425 - val_loss: 5.9395 - val_acc: 0.1898\n",
      "Epoch 35/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.9622 - acc: 0.6426 - val_loss: 5.9884 - val_acc: 0.1886\n",
      "Epoch 36/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.9581 - acc: 0.6463 - val_loss: 6.0225 - val_acc: 0.1886\n",
      "Epoch 37/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.9555 - acc: 0.6460 - val_loss: 6.0809 - val_acc: 0.1867\n",
      "Epoch 38/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.9505 - acc: 0.6492 - val_loss: 6.1441 - val_acc: 0.1877\n",
      "Epoch 39/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.9454 - acc: 0.6490 - val_loss: 6.1707 - val_acc: 0.1888\n",
      "Epoch 40/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.9434 - acc: 0.6511 - val_loss: 6.2447 - val_acc: 0.1853\n",
      "Epoch 41/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.9404 - acc: 0.6495 - val_loss: 6.2578 - val_acc: 0.1867\n",
      "Epoch 42/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.9373 - acc: 0.6521 - val_loss: 6.2889 - val_acc: 0.1869\n",
      "Epoch 43/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.9350 - acc: 0.6503 - val_loss: 6.3417 - val_acc: 0.1847\n",
      "Epoch 44/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.9332 - acc: 0.6529 - val_loss: 6.3771 - val_acc: 0.1833\n",
      "Epoch 45/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.9298 - acc: 0.6547 - val_loss: 6.4140 - val_acc: 0.1850\n",
      "Epoch 46/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.9252 - acc: 0.6552 - val_loss: 6.4572 - val_acc: 0.1847\n",
      "Epoch 47/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.9233 - acc: 0.6571 - val_loss: 6.4888 - val_acc: 0.1871\n",
      "Epoch 48/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.9221 - acc: 0.6554 - val_loss: 6.5397 - val_acc: 0.1807\n",
      "Epoch 49/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.9177 - acc: 0.6598 - val_loss: 6.5900 - val_acc: 0.1818\n",
      "Epoch 50/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.9149 - acc: 0.6601 - val_loss: 6.6099 - val_acc: 0.1820\n",
      "Epoch 51/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.9132 - acc: 0.6591 - val_loss: 6.6565 - val_acc: 0.1829\n",
      "Epoch 52/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.9113 - acc: 0.6617 - val_loss: 6.6969 - val_acc: 0.1808\n",
      "Epoch 53/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.9089 - acc: 0.6616 - val_loss: 6.6863 - val_acc: 0.1819\n",
      "Epoch 54/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.9068 - acc: 0.6628 - val_loss: 6.7353 - val_acc: 0.1815\n",
      "Epoch 55/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.9057 - acc: 0.6638 - val_loss: 6.7593 - val_acc: 0.1804\n",
      "Epoch 56/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.9022 - acc: 0.6643 - val_loss: 6.7870 - val_acc: 0.1805\n",
      "Epoch 57/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.9024 - acc: 0.6619 - val_loss: 6.8213 - val_acc: 0.1786\n",
      "Epoch 58/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8973 - acc: 0.6653 - val_loss: 6.8821 - val_acc: 0.1777\n",
      "Epoch 59/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8960 - acc: 0.6681 - val_loss: 6.8876 - val_acc: 0.1809\n",
      "Epoch 60/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8929 - acc: 0.6696 - val_loss: 6.9416 - val_acc: 0.1790\n",
      "Epoch 61/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8916 - acc: 0.6702 - val_loss: 6.9464 - val_acc: 0.1780\n",
      "Epoch 62/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8908 - acc: 0.6702 - val_loss: 6.9703 - val_acc: 0.1794\n",
      "Epoch 63/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8901 - acc: 0.6683 - val_loss: 6.9820 - val_acc: 0.1807\n",
      "Epoch 64/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8870 - acc: 0.6683 - val_loss: 7.0110 - val_acc: 0.1785\n",
      "Epoch 65/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8876 - acc: 0.6703 - val_loss: 7.0399 - val_acc: 0.1787\n",
      "Epoch 66/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8853 - acc: 0.6694 - val_loss: 7.0715 - val_acc: 0.1769\n",
      "Epoch 67/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8836 - acc: 0.6696 - val_loss: 7.0777 - val_acc: 0.1779\n",
      "Epoch 68/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8794 - acc: 0.6714 - val_loss: 7.1194 - val_acc: 0.1767\n",
      "Epoch 69/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8779 - acc: 0.6740 - val_loss: 7.1246 - val_acc: 0.1788\n",
      "Epoch 70/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8767 - acc: 0.6752 - val_loss: 7.1703 - val_acc: 0.1784\n",
      "Epoch 71/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8763 - acc: 0.6767 - val_loss: 7.1841 - val_acc: 0.1754\n",
      "Epoch 72/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8741 - acc: 0.6733 - val_loss: 7.2247 - val_acc: 0.1742\n",
      "Epoch 73/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8745 - acc: 0.6778 - val_loss: 7.2295 - val_acc: 0.1765\n",
      "Epoch 74/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8707 - acc: 0.6774 - val_loss: 7.2471 - val_acc: 0.1784\n",
      "Epoch 75/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8696 - acc: 0.6747 - val_loss: 7.2809 - val_acc: 0.1740\n",
      "Epoch 76/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.8680 - acc: 0.6786 - val_loss: 7.2891 - val_acc: 0.1771\n",
      "Epoch 77/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8672 - acc: 0.6767 - val_loss: 7.3154 - val_acc: 0.1749\n",
      "Epoch 78/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8675 - acc: 0.6781 - val_loss: 7.3033 - val_acc: 0.1747\n",
      "Epoch 79/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8664 - acc: 0.6771 - val_loss: 7.3376 - val_acc: 0.1749\n",
      "Epoch 80/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8626 - acc: 0.6776 - val_loss: 7.3546 - val_acc: 0.1731\n",
      "Epoch 81/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8620 - acc: 0.6786 - val_loss: 7.3807 - val_acc: 0.1748\n",
      "Epoch 82/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8586 - acc: 0.6786 - val_loss: 7.4229 - val_acc: 0.1736\n",
      "Epoch 83/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8581 - acc: 0.6786 - val_loss: 7.4311 - val_acc: 0.1741\n",
      "Epoch 84/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8586 - acc: 0.6804 - val_loss: 7.4430 - val_acc: 0.1741\n",
      "Epoch 85/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8564 - acc: 0.6805 - val_loss: 7.4393 - val_acc: 0.1753\n",
      "Epoch 86/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.8553 - acc: 0.6813 - val_loss: 7.5008 - val_acc: 0.1717\n",
      "Epoch 87/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8530 - acc: 0.6818 - val_loss: 7.5075 - val_acc: 0.1739\n",
      "Epoch 88/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.8513 - acc: 0.6843 - val_loss: 7.5403 - val_acc: 0.1734\n",
      "Epoch 89/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8506 - acc: 0.6835 - val_loss: 7.5674 - val_acc: 0.1723\n",
      "Epoch 90/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8511 - acc: 0.6837 - val_loss: 7.5666 - val_acc: 0.1733\n",
      "Epoch 91/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8487 - acc: 0.6837 - val_loss: 7.5736 - val_acc: 0.1732\n",
      "Epoch 92/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8488 - acc: 0.6869 - val_loss: 7.6052 - val_acc: 0.1720\n",
      "Epoch 93/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8464 - acc: 0.6870 - val_loss: 7.6344 - val_acc: 0.1718\n",
      "Epoch 94/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.8457 - acc: 0.6837 - val_loss: 7.6379 - val_acc: 0.1717\n",
      "Epoch 95/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.8435 - acc: 0.6886 - val_loss: 7.6465 - val_acc: 0.1727\n",
      "Epoch 96/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.8443 - acc: 0.6873 - val_loss: 7.6872 - val_acc: 0.1713\n",
      "Epoch 97/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.8425 - acc: 0.6864 - val_loss: 7.6878 - val_acc: 0.1711\n",
      "Epoch 98/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.8414 - acc: 0.6866 - val_loss: 7.7099 - val_acc: 0.1720\n",
      "Epoch 99/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.8394 - acc: 0.6881 - val_loss: 7.7143 - val_acc: 0.1705\n",
      "Epoch 100/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8394 - acc: 0.6866 - val_loss: 7.7494 - val_acc: 0.1694\n",
      "Epoch 101/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8389 - acc: 0.6879 - val_loss: 7.7375 - val_acc: 0.1719\n",
      "Epoch 102/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8383 - acc: 0.6854 - val_loss: 7.7580 - val_acc: 0.1714\n",
      "Epoch 103/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8355 - acc: 0.6911 - val_loss: 7.7953 - val_acc: 0.1701\n",
      "Epoch 104/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8337 - acc: 0.6892 - val_loss: 7.7977 - val_acc: 0.1699\n",
      "Epoch 105/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8337 - acc: 0.6910 - val_loss: 7.8287 - val_acc: 0.1700\n",
      "Epoch 106/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.8325 - acc: 0.6908 - val_loss: 7.8139 - val_acc: 0.1694\n",
      "Epoch 107/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8331 - acc: 0.6901 - val_loss: 7.8349 - val_acc: 0.1706\n",
      "Epoch 108/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8326 - acc: 0.6923 - val_loss: 7.8585 - val_acc: 0.1709\n",
      "Epoch 109/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8308 - acc: 0.6906 - val_loss: 7.8638 - val_acc: 0.1706\n",
      "Epoch 110/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8267 - acc: 0.6944 - val_loss: 7.8888 - val_acc: 0.1690\n",
      "Epoch 111/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8265 - acc: 0.6918 - val_loss: 7.9007 - val_acc: 0.1707\n",
      "Epoch 112/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8277 - acc: 0.6953 - val_loss: 7.9214 - val_acc: 0.1676\n",
      "Epoch 113/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.8272 - acc: 0.6936 - val_loss: 7.9131 - val_acc: 0.1698\n",
      "Epoch 114/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8250 - acc: 0.6924 - val_loss: 7.9444 - val_acc: 0.1693\n",
      "Epoch 115/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8233 - acc: 0.6938 - val_loss: 7.9571 - val_acc: 0.1683\n",
      "Epoch 116/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8226 - acc: 0.6953 - val_loss: 7.9641 - val_acc: 0.1691\n",
      "Epoch 117/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.8220 - acc: 0.6936 - val_loss: 7.9931 - val_acc: 0.1671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8205 - acc: 0.6941 - val_loss: 8.0020 - val_acc: 0.1676\n",
      "Epoch 119/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8208 - acc: 0.6919 - val_loss: 8.0272 - val_acc: 0.1673\n",
      "Epoch 120/200\n",
      "20164/20164 [==============================] - 1s 38us/step - loss: 0.8203 - acc: 0.6945 - val_loss: 8.0113 - val_acc: 0.1683\n",
      "Epoch 121/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8182 - acc: 0.6978 - val_loss: 8.0294 - val_acc: 0.1670\n",
      "Epoch 122/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.8216 - acc: 0.6932 - val_loss: 8.0178 - val_acc: 0.1682\n",
      "Epoch 123/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.8205 - acc: 0.6945 - val_loss: 8.0616 - val_acc: 0.1683\n",
      "Epoch 124/200\n",
      "20164/20164 [==============================] - 1s 35us/step - loss: 0.8167 - acc: 0.6987 - val_loss: 8.0543 - val_acc: 0.1683\n",
      "Epoch 125/200\n",
      "20164/20164 [==============================] - 1s 34us/step - loss: 0.8156 - acc: 0.6964 - val_loss: 8.0679 - val_acc: 0.1672\n",
      "Epoch 126/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8159 - acc: 0.6976 - val_loss: 8.0689 - val_acc: 0.1669\n",
      "Epoch 127/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8137 - acc: 0.6949 - val_loss: 8.0903 - val_acc: 0.1678\n",
      "Epoch 128/200\n",
      "20164/20164 [==============================] - 1s 36us/step - loss: 0.8142 - acc: 0.6969 - val_loss: 8.1195 - val_acc: 0.1657\n",
      "Epoch 129/200\n",
      "20164/20164 [==============================] - 1s 35us/step - loss: 0.8115 - acc: 0.7012 - val_loss: 8.1491 - val_acc: 0.1666\n",
      "Epoch 130/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8131 - acc: 0.6994 - val_loss: 8.1337 - val_acc: 0.1663\n",
      "Epoch 131/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.8117 - acc: 0.6992 - val_loss: 8.1383 - val_acc: 0.1687\n",
      "Epoch 132/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8106 - acc: 0.6978 - val_loss: 8.1444 - val_acc: 0.1675\n",
      "Epoch 133/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.8089 - acc: 0.6988 - val_loss: 8.1683 - val_acc: 0.1668\n",
      "Epoch 134/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8088 - acc: 0.6997 - val_loss: 8.1513 - val_acc: 0.1659\n",
      "Epoch 135/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.8085 - acc: 0.6981 - val_loss: 8.1964 - val_acc: 0.1654\n",
      "Epoch 136/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8079 - acc: 0.7012 - val_loss: 8.2032 - val_acc: 0.1665\n",
      "Epoch 137/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8086 - acc: 0.6995 - val_loss: 8.2293 - val_acc: 0.1655\n",
      "Epoch 138/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8032 - acc: 0.7012 - val_loss: 8.2322 - val_acc: 0.1639\n",
      "Epoch 139/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.8066 - acc: 0.7027 - val_loss: 8.2251 - val_acc: 0.1661\n",
      "Epoch 140/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8050 - acc: 0.6999 - val_loss: 8.2388 - val_acc: 0.1676\n",
      "Epoch 141/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8041 - acc: 0.7000 - val_loss: 8.2554 - val_acc: 0.1649\n",
      "Epoch 142/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.8032 - acc: 0.7023 - val_loss: 8.2666 - val_acc: 0.1665\n",
      "Epoch 143/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.8042 - acc: 0.6998 - val_loss: 8.2660 - val_acc: 0.1660\n",
      "Epoch 144/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.8026 - acc: 0.6987 - val_loss: 8.3206 - val_acc: 0.1622\n",
      "Epoch 145/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.8005 - acc: 0.7032 - val_loss: 8.2812 - val_acc: 0.1656\n",
      "Epoch 146/200\n",
      "20164/20164 [==============================] - 1s 37us/step - loss: 0.7995 - acc: 0.7022 - val_loss: 8.3002 - val_acc: 0.1654\n",
      "Epoch 147/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.7999 - acc: 0.7031 - val_loss: 8.3028 - val_acc: 0.1663\n",
      "Epoch 148/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7984 - acc: 0.7029 - val_loss: 8.3177 - val_acc: 0.1657\n",
      "Epoch 149/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7981 - acc: 0.7028 - val_loss: 8.3335 - val_acc: 0.1652\n",
      "Epoch 150/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.7984 - acc: 0.7039 - val_loss: 8.3411 - val_acc: 0.1639\n",
      "Epoch 151/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7968 - acc: 0.7036 - val_loss: 8.3296 - val_acc: 0.1657\n",
      "Epoch 152/200\n",
      "20164/20164 [==============================] - 1s 35us/step - loss: 0.7996 - acc: 0.7015 - val_loss: 8.3612 - val_acc: 0.1641\n",
      "Epoch 153/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.7951 - acc: 0.7056 - val_loss: 8.3685 - val_acc: 0.1637\n",
      "Epoch 154/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7954 - acc: 0.7059 - val_loss: 8.3721 - val_acc: 0.1648\n",
      "Epoch 155/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.7942 - acc: 0.7048 - val_loss: 8.3909 - val_acc: 0.1616\n",
      "Epoch 156/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.7956 - acc: 0.7038 - val_loss: 8.4081 - val_acc: 0.1635\n",
      "Epoch 157/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.7947 - acc: 0.7042 - val_loss: 8.3940 - val_acc: 0.1659\n",
      "Epoch 158/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7949 - acc: 0.7036 - val_loss: 8.4240 - val_acc: 0.1626\n",
      "Epoch 159/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.7948 - acc: 0.7042 - val_loss: 8.4183 - val_acc: 0.1637\n",
      "Epoch 160/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.7914 - acc: 0.7054 - val_loss: 8.4248 - val_acc: 0.1624\n",
      "Epoch 161/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.7879 - acc: 0.7099 - val_loss: 8.4328 - val_acc: 0.1635\n",
      "Epoch 162/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.7898 - acc: 0.7069 - val_loss: 8.4435 - val_acc: 0.1639\n",
      "Epoch 163/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.7893 - acc: 0.7087 - val_loss: 8.4541 - val_acc: 0.1633\n",
      "Epoch 164/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.7917 - acc: 0.7051 - val_loss: 8.4453 - val_acc: 0.1633\n",
      "Epoch 165/200\n",
      "20164/20164 [==============================] - 1s 34us/step - loss: 0.7893 - acc: 0.7093 - val_loss: 8.4832 - val_acc: 0.1620\n",
      "Epoch 166/200\n",
      "20164/20164 [==============================] - 1s 39us/step - loss: 0.7882 - acc: 0.7056 - val_loss: 8.4687 - val_acc: 0.1628\n",
      "Epoch 167/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7877 - acc: 0.7074 - val_loss: 8.4765 - val_acc: 0.1640\n",
      "Epoch 168/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.7849 - acc: 0.7087 - val_loss: 8.4898 - val_acc: 0.1616\n",
      "Epoch 169/200\n",
      "20164/20164 [==============================] - 1s 25us/step - loss: 0.7870 - acc: 0.7093 - val_loss: 8.4927 - val_acc: 0.1630\n",
      "Epoch 170/200\n",
      "20164/20164 [==============================] - 1s 26us/step - loss: 0.7849 - acc: 0.7094 - val_loss: 8.5100 - val_acc: 0.1632\n",
      "Epoch 171/200\n",
      "20164/20164 [==============================] - 1s 27us/step - loss: 0.7863 - acc: 0.7087 - val_loss: 8.5300 - val_acc: 0.1617\n",
      "Epoch 172/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.7832 - acc: 0.7082 - val_loss: 8.5318 - val_acc: 0.1624\n",
      "Epoch 173/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7854 - acc: 0.7080 - val_loss: 8.5177 - val_acc: 0.1634\n",
      "Epoch 174/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7832 - acc: 0.7113 - val_loss: 8.5379 - val_acc: 0.1608\n",
      "Epoch 175/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.7814 - acc: 0.7101 - val_loss: 8.5568 - val_acc: 0.1600\n",
      "Epoch 176/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7822 - acc: 0.7075 - val_loss: 8.5512 - val_acc: 0.1628\n",
      "Epoch 177/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7815 - acc: 0.7103 - val_loss: 8.5594 - val_acc: 0.1613\n",
      "Epoch 178/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7833 - acc: 0.7140 - val_loss: 8.5763 - val_acc: 0.1612\n",
      "Epoch 179/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7798 - acc: 0.7096 - val_loss: 8.5670 - val_acc: 0.1626\n",
      "Epoch 180/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7792 - acc: 0.7103 - val_loss: 8.5935 - val_acc: 0.1595\n",
      "Epoch 181/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7814 - acc: 0.7085 - val_loss: 8.5951 - val_acc: 0.1612\n",
      "Epoch 182/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.7784 - acc: 0.7130 - val_loss: 8.6076 - val_acc: 0.1630\n",
      "Epoch 183/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7784 - acc: 0.7108 - val_loss: 8.5959 - val_acc: 0.1622\n",
      "Epoch 184/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7772 - acc: 0.7091 - val_loss: 8.6002 - val_acc: 0.1629\n",
      "Epoch 185/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.7803 - acc: 0.7100 - val_loss: 8.5997 - val_acc: 0.1618\n",
      "Epoch 186/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7768 - acc: 0.7141 - val_loss: 8.6285 - val_acc: 0.1611\n",
      "Epoch 187/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.7788 - acc: 0.7117 - val_loss: 8.6500 - val_acc: 0.1603\n",
      "Epoch 188/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7731 - acc: 0.7121 - val_loss: 8.6423 - val_acc: 0.1604\n",
      "Epoch 189/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7755 - acc: 0.7119 - val_loss: 8.6524 - val_acc: 0.1608\n",
      "Epoch 190/200\n",
      "20164/20164 [==============================] - 1s 29us/step - loss: 0.7748 - acc: 0.7121 - val_loss: 8.6583 - val_acc: 0.1614\n",
      "Epoch 191/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7762 - acc: 0.7145 - val_loss: 8.6616 - val_acc: 0.1612\n",
      "Epoch 192/200\n",
      "20164/20164 [==============================] - 1s 31us/step - loss: 0.7755 - acc: 0.7108 - val_loss: 8.6754 - val_acc: 0.1618\n",
      "Epoch 193/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.7768 - acc: 0.7126 - val_loss: 8.6847 - val_acc: 0.1604\n",
      "Epoch 194/200\n",
      "20164/20164 [==============================] - 1s 28us/step - loss: 0.7748 - acc: 0.7157 - val_loss: 8.6735 - val_acc: 0.1603\n",
      "Epoch 195/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.7726 - acc: 0.7155 - val_loss: 8.6832 - val_acc: 0.1601\n",
      "Epoch 196/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7717 - acc: 0.7126 - val_loss: 8.6699 - val_acc: 0.1628\n",
      "Epoch 197/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.7725 - acc: 0.7124 - val_loss: 8.6982 - val_acc: 0.1596\n",
      "Epoch 198/200\n",
      "20164/20164 [==============================] - 1s 32us/step - loss: 0.7702 - acc: 0.7147 - val_loss: 8.7145 - val_acc: 0.1596\n",
      "Epoch 199/200\n",
      "20164/20164 [==============================] - 1s 30us/step - loss: 0.7693 - acc: 0.7157 - val_loss: 8.7148 - val_acc: 0.1567\n",
      "Epoch 200/200\n",
      "20164/20164 [==============================] - 1s 33us/step - loss: 0.7699 - acc: 0.7188 - val_loss: 8.7249 - val_acc: 0.1601\n"
     ]
    }
   ],
   "source": [
    "myRator.train(x=[X1,X2],y=Y,batch_size=256,epochs=200,validation_split=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
